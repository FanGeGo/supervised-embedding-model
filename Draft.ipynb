{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def score(context, response):\n",
    "    return random.random()\n",
    "\n",
    "\n",
    "def best_response(context, candidates):\n",
    "    index = np.argmax([score(context, response) for response in candidates])\n",
    "    return candidates[index]\n",
    "\n",
    "\n",
    "def parse_dialogs(filename):\n",
    "    dialogs = []\n",
    "    with open(filename, 'r') as f:\n",
    "        dialog = []\n",
    "        for line in f:\n",
    "            if line.strip() == '':\n",
    "                dialogs.append(dialog)\n",
    "                dialog = []\n",
    "            else:\n",
    "                user_utt, bot_utt = line.strip().split('\\t')\n",
    "                utt_num = user_utt.split(' ')[0]\n",
    "                user_utt = ' '.join(user_utt.split(' ')[1:])\n",
    "                dialog.append((utt_num, user_utt, bot_utt))\n",
    "    return dialogs\n",
    "\n",
    "\n",
    "def parse_candidates(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return [' '.join(line.strip().split(' ')[1:]) for line in f]            \n",
    "\n",
    "    \n",
    "def responses_accuracy(dialogs, candidates):\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    for dialog in dialogs:\n",
    "        for _, user_utt, bot_utt in dialog:\n",
    "            count += 1\n",
    "            context = user_utt\n",
    "            response = best_response(context, candidates)\n",
    "            if response == bot_utt:\n",
    "                correct += 1\n",
    "    return correct / count, correct, count\n",
    "\n",
    "\n",
    "def build_vocab_to_ind_map(dialogs):\n",
    "    vocab = set()\n",
    "    for d in dialogs:\n",
    "        for _, user_utt, bot_utt in d:\n",
    "            vocab = vocab.union(user_utt.split(' ') + bot_utt.split(' '))\n",
    "    vocab = sorted(list(vocab))\n",
    "    \n",
    "    cntr = 0\n",
    "    vocab_ind_map = {}\n",
    "    for w in vocab:\n",
    "        vocab_ind_map[w] = cntr\n",
    "        cntr += 1\n",
    "    return vocab_ind_map\n",
    "\n",
    "\n",
    "def build_vec(vocab_ind_map, utt):\n",
    "    vocab_len = len(vocab_ind_map.keys())\n",
    "    vec = np.zeros((vocab_len, 1))\n",
    "    for w in utt.split(' '):\n",
    "        vec[vocab_ind_map[w]] += 1\n",
    "    return vec\n",
    "\n",
    "\n",
    "def get_vec_set(dialogs, vocab_ind_map):\n",
    "    vec_set = []\n",
    "    for d in dialogs:\n",
    "        for _, user_utt, bot_utt in d:\n",
    "            x = build_vec(vocab_ind_map, user_utt)\n",
    "            y = build_vec(vocab_ind_map, bot_utt)\n",
    "            vec_set.append([x, y])\n",
    "    return vec_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_set_task1_dialogs = parse_dialogs('dataset/dialog-bAbI-tasks/dialog-babi-task1-API-calls-trn.txt')\n",
    "dev_set_task1_dialogs = parse_dialogs('dataset/dialog-bAbI-tasks/dialog-babi-task1-API-calls-dev.txt')\n",
    "candidates = parse_candidates('dataset/dialog-bAbI-tasks/dialog-babi-candidates.txt')\n",
    "vocab_to_ind_map = build_vocab_to_ind_map(train_set_task1_dialogs)\n",
    "vec_set = get_vec_set(train_set_task1_dialogs, vocab_to_ind_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "D = 20\n",
    "V = len(vocab_to_ind_map)\n",
    "\n",
    "context = tf.placeholder(dtype=tf.float32, name='Context', shape=[V, 1])\n",
    "neg_context = tf.placeholder(dtype=tf.float32, name='NegativeContext', shape=[V, 1])\n",
    "response = tf.placeholder(dtype=tf.float32, name='Response', shape=[V, 1])\n",
    "A_var = tf.Variable(tf.random_uniform([D, V], -1.0, 1.0))\n",
    "B_var = tf.Variable(tf.random_uniform([D, V], -1.0, 1.0))\n",
    "\n",
    "f_pos = tf.matmul(tf.transpose(tf.matmul(A_var, context)), tf.matmul(B_var, response))\n",
    "f_neg = tf.matmul(tf.transpose(tf.matmul(A_var, neg_context)), tf.matmul(B_var, response))\n",
    "\n",
    "m = 0.01\n",
    "loss = tf.nn.relu(f_pos - f_neg + m)\n",
    "\n",
    "LR = 0.001\n",
    "optimizer = tf.train.GradientDescentOptimizer(LR).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c65d471d7b94002b2a572f20615d88e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da5cbeae45f45c08ae0928dfa32c228"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[array([[ 0.]], dtype=float32), None] [ 1.06638396]\n",
      "[array([[-6.5842762]], dtype=float32)] [array([[-3.84863114]], dtype=float32)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b67e7f243f949478c24deaf6aec5941"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.]], dtype=float32), None] [ 0.08665579]\n",
      "[array([[-8.40829372]], dtype=float32)] [array([[-1.92801046]], dtype=float32)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "374e8a69dda940918d5ab03d0bfe0691"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.]], dtype=float32), None] [ 0.01955559]\n",
      "[array([[-8.96854973]], dtype=float32)] [array([[-1.16675735]], dtype=float32)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba7b90f190b4bdf9aefc0fbbd6b89c9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.]], dtype=float32), None] [ 0.01130519]\n",
      "[array([[-9.40524387]], dtype=float32)] [array([[-0.67194474]], dtype=float32)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36861a4031834c2997633f5dba650c4d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.]], dtype=float32), None] [ 0.0103422]\n",
      "[array([[-9.55266953]], dtype=float32)] [array([[-0.52301461]], dtype=float32)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddff09ebb92044008690819bcef066b2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.]], dtype=float32), None] [ 0.00429054]\n",
      "[array([[-9.6255188]], dtype=float32)] [array([[-0.49306503]], dtype=float32)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4efb3938744348fea73d7c35de22cf12"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.]], dtype=float32), None] [ 0.00364747]\n",
      "[array([[-9.76230431]], dtype=float32)] [array([[-0.37666884]], dtype=float32)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "921ee793d20a4585bdb64e5f74227572"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.]], dtype=float32), None] [ 0.00326872]\n",
      "[array([[-9.89340878]], dtype=float32)] [array([[-0.23280609]], dtype=float32)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339a3d05f9bc4c738ceaadafd498215c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.]], dtype=float32), None] [ 0.0029568]\n",
      "[array([[-9.98664951]], dtype=float32)] [array([[-0.16421798]], dtype=float32)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d8575a3b9224b81858afdc5188720c9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.]], dtype=float32), None] [ 0.00120396]\n",
      "[array([[-9.99772358]], dtype=float32)] [array([[-0.1633212]], dtype=float32)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    avg_loss = 0.0\n",
    "    for _ in tnrange(10):\n",
    "        for x, y in tqdm_notebook(vec_set):\n",
    "            y_negs = random.sample(vec_set, 1)\n",
    "            for _, y_neg in y_negs:\n",
    "                loss_elem = sess.run([loss, optimizer], feed_dict={context: x, response: y, neg_context: y_neg})\n",
    "                avg_loss += loss_elem[0][0]\n",
    "        avg_loss = avg_loss / len(vec_set)\n",
    "        print(loss_elem, avg_loss)\n",
    "        val_pos = sess.run([f_pos], feed_dict={context: vec_set[-1][0], response: vec_set[-1][1]})\n",
    "        val_neg = sess.run([f_neg], feed_dict={neg_context: vec_set[1][0], response: vec_set[-1][1]})\n",
    "        print(val_pos, val_neg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
